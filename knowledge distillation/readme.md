# 知识蒸馏（后面可能会改成模型压缩）

这个repo主要用于实现一个简单的知识蒸馏框架，随着实验的推进来扩充readme的内容。
当然后期可能会改成模型压缩的repo，那就是后话了。

所以这里先提一嘴深度可分离卷积（depthwise separable convolution）。
这里的“深度”指的其实是channel，也就是在通道上做文章。

## 标准卷积

在标准的卷积操作中，卷积核的通道数和input的通道数是相同的，例如对RGB三通道图像的卷积，卷积过程中将三个通道的权重和相加，得到一个output。

以下图为例，一个12×12×3的输入，用5×5×3的卷积核进行卷积，一次卷积得到8×8×1的output，那么256个卷积核进行256次卷积就能得到8×8×256的output。

来计算一下这波卷积中的参数量，params = 5×5×3×256=19200

<img src="https://pic4.zhimg.com/80/v2-f471bdb9191d0c8b65688ececbe935fb_1440w.webp" title="" alt="" data-align="center">

## 深度可分离卷积

那么接下来来看深度可分离卷积。

深度可分离卷积其实就是把标准的卷积操作分为两个步骤完成，分别是**深度卷积**和**逐点卷积**。

### 深度卷积

深度卷积和标准卷积的区别在于，深度卷积的卷积核只有1个通道，但有n（取决于）个单通道的卷积核，每一次卷积操作不改变通道的维度，只对当前通道进行卷积。

如下图所示

![](https://pic2.zhimg.com/80/v2-b74a5e8241eb500949d8dcc47558d035_1440w.webp)

一次深度卷积产生的参数量为 params = 5×5×3 = 75。

### 逐点卷积

在经过深度卷积之后，使用m个1×1×n的卷积核，对深度卷积得到的中间map进行逐点卷积。

如下图所示，最后得到一个和标准卷积相同尺寸的feature map。

![](https://pic1.zhimg.com/80/v2-f480c4453e9b7915c88d34ca79288e20_1440w.webp)

以上图为例，逐点卷积产生的参数量显然就是1×1×3×256=768



那么，深度可分离卷积产生的参数量就是75+768=843，远远小于标准卷积的19200。



对上述理论做一个总结

![](https://pic2.zhimg.com/80/v2-eb2f07d6b8bc4c8a90c5daafbba683dd_1440w.webp)

![](https://pic1.zhimg.com/80/v2-2d11a371ccccc4716958e752ce6d423c_1440w.webp)

无论是参数量，还是计算量，都有了非常显著的减少。

但是，参数量和计算量是减少了，效果呢？其实从理论上来看，深度卷积和逐点卷积的组合并没有丢失什么信息，硬要说的话，也说不出来什么。所以直接跑个实验看看吧。
